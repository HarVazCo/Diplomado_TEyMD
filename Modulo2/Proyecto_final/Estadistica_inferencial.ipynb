{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadística Inferencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hay muchas formas de tomar decisiones o predicciones, algunas son subjetivas y otras son objetivas por naturaleza. ¿Qué tan buenas serán las predicciones o decisiones? Es la función del estadístico matemático dar métodos de toma de inferencia estadística que son mejores y más confiables que únicamente cálculos subjetivos.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores\n",
    "\n",
    "Cuando se quiere estimar el valor de un parámetro poblacional, se puede usar información de la muestra en la forma de un *estimador*. \n",
    "\n",
    "*Estimadores:* es una función de la muestra aleatoria que se usa para aproximar un parámetro desconocido de la población.\n",
    "\n",
    "### Propiedades de los estimadores\n",
    "\n",
    "- **Insesgamiento:** \n",
    "  \n",
    "  Se dice que un estimador de un parámetro es insesgado si la media de su distribución es igual al verdadero valor del parámetro. El estimador no debe subestimar o sobreestimar de manera consistente el parámetro de interés. De otro modo, se dice que el estimador está sesgado.\n",
    "  $$ \\mathbb{E}[\\hat{\\theta}] = \\theta$$\n",
    "\n",
    "- **Consistencia:**\n",
    "  \n",
    "  Esta propiedad considera que el estimador converge en probabilidad al parámetro cuando el tamaño de la muestra crece a infinito. Es decir, si $\\hat{\\theta}_n$ es un estimador para $\\theta$ basado en una m.a. de tamaño n. $\\hat{\\theta}_n$ es consistente si para cualquier $\\varepsilon > 0$\n",
    "  $$ \\lim_{n\\to\\infty} \\mathbb{P}(|\\hat{\\theta}_n - \\theta|>\\varepsilon) = 0$$\n",
    "  otra notación es $\\hat{\\theta}_n \\xrightarrow{p}\\theta$\n",
    "\n",
    "- **Eficiencia:**\n",
    "\n",
    "  La dispersión (medida por la varianza) de la distribución muestral debe ser tan pequeña como sea posible. Esto asegura que, con una alta probabilidad, una estimación individual caerá cerca del valor verdadero del parámetro.\n",
    "\n",
    "### Métodos para construir estimadores\n",
    "\n",
    "#### Método de momentos \n",
    "\n",
    "Consiste en igualar los momentos poblacionales con los momentos muestrales y resolver la ecuación, o sistema de ecuaciones , para el parámetro o vector de parámetros, cuando sea posible. \n",
    " \n",
    " - Momento poblacional: Sea $k \\geq 1$ un entero. El $k$-ésimo momento de una variable aleatoria $X$, es el número $\\mathbb{E}[X^k]$.\n",
    " - Momento muestral: Sea $k \\geq 1$ un entero. El $k$-ésimo momento de una muestra aleatoria $X_1,\\dots,X_n$ es la variable aleatoria $m_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k$_\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "El tiempo de retardo para pacientes con enfermedad coronaria potencial se define como la duración entre el comienzo de un síntoma agudo cardiovascular y la llegada a la unidad de emergencia.\n",
    "Se supone que el tiempo de retardo sigue una distribución exponencial con parámetro $\\theta $.\n",
    "Se registraron los tiempos de retardo (en minutos) en la clínica para los primeros 20 pacientes:\n",
    "$$ 525, \\ 719, \\ 2880, \\ 150, \\ 30, \\ 251, \\ 45, \\ 858, \\ 15, \\ 47, \\ 90, \\ 56, \\ 68, \\ 6, \\ 189, \\ 180, \\ 60, \\ 60, \\ 294, \\ 747 $$\n",
    "- Encuentra un estimador por el **método de momentos** para la media de la distribución (exponencial).\n",
    "  Recordemos que si $X\\sim Exp(\\theta)$, entonces $\\mathbb{E}[X]=\\frac{1}{\\theta}$\n",
    "- (Python) Encuentra el valor del estimador con los datos dados.\n",
    "\n",
    "**Inciso 1**\n",
    "\n",
    "Sabemos que para una distribución exponencial $\\mathbb{E}[X] = \\frac{1}{\\theta}$ y que el momento muestral $m_1 = \\frac{1}{n}\\sum_{i=1}^{n}x_i$, igualando los momentos obtenemos \n",
    "$$ \\frac{1}{\\theta} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x}$$\n",
    "y despejando, llegamos al estimador \n",
    "$$ \\hat{\\theta} = \\frac{1}{\\bar{x}}$$\n",
    "\n",
    "**Inciso 2**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor del estimador es: 0.002751031636863824\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a las librerias que ocuparemos\n",
    "import numpy as np\n",
    "\n",
    "# Creamos un arreglo con los datos \n",
    "datos = np.array([525, 719, 2880, 150, 30, 251, 45, 858, 15, 47, \n",
    "                 90, 56, 68, 6, 189, 180, 60, 60, 294, 747])\n",
    "\n",
    "# Calculamos la media \n",
    "media = np.mean(datos)\n",
    "\n",
    "# Calculamos el estimador\n",
    "theta_hat = 1/media\n",
    "\n",
    "# Imprimimos el valor del estimador\n",
    "print('El valor del estimador es:',theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método de máxima verosimilitud\n",
    "\n",
    "La *función de verosimilitud* de un vector aleatorio $ (X_1,\\dots,X_n)$ cuya distribución depende de un parámetro $\\theta$ se define como la función de densidad \n",
    "$$L(\\theta) = f_{X_1,\\dots,X_n}(x_1,\\dots,x_n;\\theta) $$\n",
    "y cuando hay distribución idéntica de $X_i$ se calcula de la siguiente manera\n",
    "$$ L(\\theta) = f(x_1;\\theta)\\cdots f(x_n;\\theta) = \\prod_{i=1}^{n}f(x_i;\\theta).$$\n",
    "\n",
    "El método consiste en encontrar el valor de $\\theta$ que maximiza a la función $L(\\theta)$. Al valor de $\\theta$ en donde $L(\\theta)$ alcanza su máximo se le llama estimación de máxima verosimilitud o estimación máximo verosímil.\n",
    "\n",
    "**Observación**\n",
    "$$\\begin{align*} \\log{L(\\theta)} &=  \\log{\\prod_{i=1}^{n}f(x_i;\\theta)} \\\\ &= \\sum_{i=1}^{n} \\log{f(x_i;\\theta)} \\end{align*}$$\n",
    "\n",
    "**Ejemplo.** \n",
    "\n",
    "Sea $x_1,\\dots,x_n$ una m.a $Poisson(\\lambda)$\n",
    "$$ f(x_i;\\lambda) = e^{-\\lambda}\\frac{\\lambda^{x_i}}{x_i !}, \\quad i\\in\\{1,2,\\dots,n\\}$$\n",
    "1. Encontrar EMV\n",
    "2. ¿es insesgado?\n",
    "3. ECM\n",
    "\n",
    "**Inciso 1**\n",
    "\n",
    "$$L(x_1\\dots x_n;\\theta) = \\prod_{i=1}^{n} e^{-\\theta}\\frac{\\theta^{x_i}}{x_i !} = \\frac{e^{-n\\theta}\\theta^{\\sum x_i}}{x_1!\\dots x_n!}$$\n",
    "\n",
    "$$\\log{L(x_1\\dots x_n;\\theta)} = \\log{e^{-n\\theta}\\theta^{\\sum x_i}} - \\log{x_1!\\dots x_n!} = -n\\theta + \\sum x_i \\log{\\theta} - \\sum \\log{x_i!}$$\n",
    "derivando $L(x_i;\\theta)$ e igualando a cero\n",
    "$$ \\frac{dL(x_i;\\theta)}{d\\theta} = -n + \\frac{\\sum x_i}{\\theta} = 0$$\n",
    "así\n",
    "$$ \\frac{\\sum x_i}{\\theta} = n \\rightarrow  \\theta = \\frac{\\sum x_i}{n} \\quad \\text{entonces} \\quad \\hat{\\theta} = \\bar{x}$$\n",
    "\n",
    "**Inciso 2**\n",
    "$$ \\mathbb{E}[\\hat{\\theta}] = \\mathbb{E}\\left[\\frac{\\sum x_i}{n}\\right] = \\frac{1}{n}\\sum \\mathbb{E}[x_i] = \\frac{1}{n}\\sum \\theta = \\frac{n\\theta}{n} = \\theta $$\n",
    "\n",
    "**Inciso 3**\n",
    "$$\\begin{align*}ECM(\\hat{\\theta}) &= Var(\\hat{\\theta}) + (\\mathbb{E}[\\hat{\\theta}]-\\theta)^2 = Var\\left(\\frac{\\sum x_i}{n} \\right) + 0 \\\\ &= \\frac{1}{n^2} \\sum Var(x_i) = \\frac{1}{n^2} \\sum \\theta = \\frac{n\\theta}{n^2} = \\frac{\\theta}{n} \\end{align*}$$\n",
    "disminuye a medida que aumenta la cantidad de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confianza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
